env:
  name: "GridLife-v0"
  horizon: 512
  partial_observability: true
internal_state:
  dims: ["energy","temp","integrity"]
  mu: [0.7, 0.5, 0.9]
  w:  [1.0, 0.5, 1.5]
viability:
  constraints:
    - "energy >= 0.2"
    - "temp in [0.3, 0.7]"
    - "integrity >= 0.6"
  shield:
    conf: 0.97
    horizon: 8
rewards:
  lambda_homeo: 0.7
  lambda_intr:  0.3
  intrinsic: "empowerment"  # or "surprise" or "rnd"
training:
  # --- Single-agent settings ---
  algo: "SAC"
  update_every: 64
  model_rollouts: 5
  amortize_after_steps: 20000

  # --- Multi-agent settings ---
  scheme: "ctde"
  buffer:
    type: "joint"
    capacity: 500000
    seq_len: 1

  # --- Common settings ---
  batch_size: 1024
  gamma: 0.99
  tau: 0.005 # For SAC target network updates
  actor_lr: 3e-4
  critic_lr: 3e-4

risk_sensitive:
  enabled: false # Set to true to use CVAR-SAC
  n_quantiles: 32
  tau: 0.1 # Risk level for CVaR (0.0 to 1.0). Lower is more risk-averse.

curriculum:
  enabled: true
  steps: 50000
  lambda_homeo:
    start: 0.1
    end: 0.7
  lambda_intr:
    start: 0.05
    end: 0.3
  viability_constraints:
    - dim_name: "energy"
      comparison: ">="
      start: 0.05
      end: 0.2
    - dim_name: "temp_min"
      comparison: ">="
      start: 0.1
      end: 0.3
    - dim_name: "temp_max"
      comparison: "<="
      start: 0.9
      end: 0.7
    - dim_name: "integrity"
      comparison: ">="
      start: 0.3
      end: 0.6

safety_network:
  hidden_dim: 128
  lr: 0.0001

demonstrations:
  filepath: "demonstrations/expert_demonstrations.npz"

state_estimator:
  hidden_dim: 128
  n_layers: 2
  lr: 0.001
  sequence_length: 16

empowerment:
  k: 4
  hidden_dim: 256
  lr: 0.0001

meta_learning:
  enabled: true
  learning_rate: 0.01
  update_frequency: 50 # Update mu every 50 episodes

mpc:
  enabled: false # Set to true to use the MPC agent
  horizon: 12
  num_candidates: 1000
  top_k: 100
  iterations: 10

safety:
  cbf:
    enabled: false # Set to true to enable the CBF safety layer
    relax_penalty: 10.0
    delta: 1.0
  chance_constraint:
    enabled: true
    target_violation_rate: 0.005
    dual_lr: 5e-4

ood:
  enabled: true
  method: "energy"   # energy|mc_dropout|ensemble
  threshold: -5.0

continual:
  enabled: true
  ewc_lambda: 5.0
  consolidate_every: 5000 # In steps
  rehearsal_capacity: 2000

budgets:
  enabled: false # Set to true to enable the budget meter
  initial_budget: 1.0 # The starting budget for each episode.
  decrement_per_step: 0.002 # The amount to decrement the budget by at each step. (1.0 / 500 steps)
  penalty: -1.0 # The penalty applied when the budget is exhausted.

population:
  ensemble_shield:
    enabled: false # Set to true to use the EnsembleShield
    ensemble_size: 3
    vote_method: "veto_if_any_unsafe" # or "majority_vote"

maintenance:
  enabled: true
  tasks: ["refuel", "cool_down", "repair"]
  penalty_costs: {refuel: 0.1, cool_down: 0.1, repair: 0.3}

adversarial:
  enabled: true
  epsilon: 0.03
  alpha: 0.01
  num_iter: 7

safety_probe:
  enabled: true
  log_path: "safety_reports.log"
  hidden_sizes: [64, 64]
  lr: 0.001

multiagent:
  enabled: false # Set to true to run multi-agent training
  num_agents: 8
  layout: "grid"
  sticky_agents: false
  remove_dead: true
  communication:
    enabled: false
    bandwidth: 8
    cost_per_bit: 0.001
  observation:
    view_radius: 5
    neighbor_radius: 3
    include_time: true
  collisions:
    single_occupancy: true
    frustration_cost: 0.01
    priority: ["integrity", "energy", "random"]
  termination:
    stop_when_all_dead: true
    stop_when_resource_exhausted: false
    max_steps: 1024

agent_types:
  default:
    count: 8
    policy: "shared_sac"
    role_embedding_dim: 8
    homeostasis:
      mu: [0.7, 0.5, 0.9]
      w:  [1.0, 0.5, 1.5]
    budgets:
      compute_ms_per_step: 10
      energy_cap: 1.0
    shield:
      mode: "ensemble"
      conf: 0.97
      horizon: 8
    cbf:
      enabled: false # Disabled for initial implementation
      relax_penalty: 10.0

resource_model:
  food:
    initial_density: 0.12
    regen_rule: "logistic"
    regen_rate: 0.02
    carrying_capacity: 1.0
  heat:
    field: "perlin"
    drift: 0.001
  hazards:
    spawn_rate: 0.0005
    decay_rate: 0.0

resource_allocator:
  mode: "proportional"
  fairness: "alpha_fair"
  alpha: 1.0
  tax: 0.02
  conflict_resolution: "priority"